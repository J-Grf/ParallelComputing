\section{Conclusion}

In this report, a FEM solver was optimized and parallelized by first applying serial optimizations with different compiler flags and then comparing two different parallelization paradigms. The performance on three different meshes was analysed and evaluated and an optimum of the thread count and core count for the different parallelization approaches was determined.

For the serial optimization, the compiler flag "-O3 -axSKYLAKE-AVX512" was found to enhance the performance of the FEM solver most, providing the shortest serial runtime. This compiler flag uses the most aggressive level of optimization for the used Intel C++ Compiler, combined with an optimized instruction set for the specific hardware, which is an Intel Platinum 8160, based on Intel's Skylake architecture, using 512-bit SIMD instructions. 

The optimal compiler flag was then used in combination with a parallelization with OpenMP. The two most time consuming loops were parallelized, with the reduction clause showing better performance opposed to the critical section. Combined with a static scheduling and default chunk size, the shortest runtime and therefore the highest speed-up was found for 8 threads on the coarse mesh, 10 threads on the medium mesh and 12 threads on the fine mesh. The highest efficiencies were achieved with 2 threads on each mesh. 

Using a one-sided communication approach with a round-robin partitioning for the parallelization with MPI, the runtime plots as well as speed-up and efficiency plots were generated. The highest speed-up was achieved with 2 cores on the coarse and medium meshes and with 4 cores on the fine mesh. With a monotonically decreasing efficiency for an increasing number of processors. The overall performance gain is poor, as the partitioning is not optimal, because the boundary surfaces between the individual partitions is not minimized. There is room for improvement using a more symmetric partitioning, like circular sections for the disk-shaped domain. The one-sided communication pattern requires looping over all processors inside a communicator, this procedure is increasing the runtime as the number of cores inside the MPI communicator is increased. For a higher number of cores a two-sided communication approach might be more efficient. Overall, the OpenMP parallelization showed the highest performance gain for the fine mesh, with a speed-up of 6.15 and an efficiency of 0.51 for 12 threads. Also for the medium mesh, the best improvement in performance was achieved with OpenMP, where as the MPI approach performed better for the coarse mesh. For further performance gain a hybrid parallelization with a combination of MPI and OpenMP would be desirable for future studies.