\section{Conclusion}

In this report, a FEM solver was optimized and parallelized by first applying serial optimizations with different comiler flags and then comparing two different parallelization paradigms. The performance on three different meshes was evaluated and an optimum of the thread count and core count for the different parallelization approaches was determined.

For the serial optimization, the compiler flag "-O3 -axSKYLAKE-AVX512" was found to enhance the performance of the FEM solver most, providing the shortest serial runtime. This compiler flag uses the most aggressive level of optimization, of the used Intel® C++ Compiler, combined with an optimized instruction set for the specific hardware, which is an Intel® Platinum 8160, based on Intel's® Skylake architecture, using 512-bit SIMD instructions. 

The optimal compiler flag was then used in combination with a parallelization with OpenMP. The two most time consuming loops were parallelized, with the reduction clause showing the better performance opposed to the critical section. Combined with a static scheduling and the default chunk size the shortest runtime and therefore the highest speed-up was found for 8 threads on the fine mesh, 10 threads on the medium mesh and 12 threads on the fine mesh. The highest efficiencies were achieved with 2 threads on each mesh. 

Using a one-sided communication approach with a round-robin partitioning for the parallelization with MPI, the runtime plots as well as speed-up and efficiency plots were derived. The highest speed-up was achieved with 2 cores on the coarse and medium meshes and with 4 cores on the fine mesh. With a monotonically decreasing efficiency for an increasing number of coarse. The overall performance gain is poor, as the partitioning is not optimal, where the boundary surfaces between the individual partitions is not minimized. There is room for improvement using a more symmetric partitioning, like circular sections for the disk-shaped domain. The one-sided communication pattern requires looping over all processors inside a communicator, this procedure is gaining duration as the number of cores inside the MPI communicator is increased. For a higher number of cores a two-sided communication approach might be more efficient. For further performance gain a hybrid parallelization with a combination of MPI and OpenMP would be desirable for future studies.