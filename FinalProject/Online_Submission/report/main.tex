\documentclass[a4paper, 11pt, oneside]{scrartcl}
\usepackage{lpp}

\usepackage{mathcomp}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{epstopdf}
\usepackage{makeidx}
\usepackage{mathtools} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{etoolbox}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes.geometric, arrows, positioning, patterns}
%\usepackage[caption=false,subrefformat=parens,labelformat=parens]{subfig}
\usepackage{nomencl}
\usepackage{doi}

\usepackage{listings}
\usepackage{xcolor}
\definecolor{commentgreen}{RGB}{2,112,10}
\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,165,0}
\definecolor{frenchplum}{RGB}{129,20,83}
\lstset { %
	language=C++,
	frame=tb,
	tabsize=4,
	showstringspaces=false,
	numbers=left,
	backgroundcolor=\color{white!5}, % set backgroundcolor
	basicstyle=\small\ttfamily,% basic font setting
	keywordstyle=\color{frenchplum}\ttfamily,
	stringstyle=\color{red},
	commentstyle=\color{commentgreen},
	morecomment=[l][\color{red}]{A)},
	morecomment=[l][\color{blue}]{B)}
%	classoffset=1,
%	otherkeywords={\#pragma},
%	morekeywords={\#pragma},
%	keywordstyle=\color{eminence},
%	classoffset=0
}

\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}%   partielle Ableitung
\newcommand{\tensor}[1]{\overline{\overline{#1}}}%      Tensor
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\refFig}[1]{Fig. \ref{#1}}
\newcommand{\refEq}[1]{Eq. (\ref{#1})}
\newcommand{\refEqs}[2]{Eqs. (\ref{#1}) and (\ref{#2})}
\newcommand{\refSec}[1]{Sec. \ref{#1}}
\newcommand{\refTab}[1]{Tab. \ref{#1}}
\newcommand{\refCh}[1]{Ch. \ref{#1}}
\newcommand{\refCode}[1]{Listing \ref{#1}}

\begin{document}

\mtitle{Final Report}

\mauthor{Parallel Computing for Computational Mechanics}

\maff{Johannes Grafen 380149}{johannes.grafen@rwth-aachen.de}

\mabstract{\blindtext}

\section{Introduction}

%\nocite{midsummer}

The parallelization of code for the simulation of complex mechanical systems is a key-topic in modern computational mechanics. Constantly increasing computing power of high-performance clusters allow for the simulation of large and complex systems. This requires an efficient parallelization approach, where different parallelization paradigms can be of use. \\

In this report the parallelization of a finite element code for the simulation of the stationary temperature distribution on a two-dimensional disk is investigated. For the shared memory approach the OpenMP library is used, where as the distributed memory approach is assessed by using a one-sided parallelization approach with MPI. The different parallelization approaches are discussed and compared for different number of CPUs. This report is structured as follows. In the second chapter \textit{Theory and Methods} are presented. The \textit{Implementation and Validation} of the presented problem is discussed in chapter 3. The results of the optimization of the runtime of the code is illustrated in chapter 4 \textit{Results} and the assessment of the different optimization techniques is discussed in chapter 5 \textit{Discussion}. Finally, this report is closed with a \textit{Conclusion} in chapter 6.

serial optimization is performed by  

\section{Theory and Methods}


- describe different meshes

\section{Implementation and Validation}

\subsection*{Parallelization with OpenMP}

A critical region is to be executed by only one thread at a time. This is important to prevent data races. A data race means that multiple threads try to access and manipulate the same variable / memory address at the same time. This might lead to unpredictable behaviour, which can result in incorrect computations. 

\begin{lstlisting}[caption={\label{Code:OpenMP} Parallelization of two most time consuming loops with implementation A and B}]
	for (int iter=0; iter<nIter; iter++)
	{
		// clear RHS MTnew
		A) #pragma omp parallel for
		for(i=0; i<nn; i++){
			MTnew[i] = 0;
		}
		A) #pragma omp parallel firstprivate(MTnew)
		B) #pragma omp parallel
		{
			// Evaluate right hand side at element level
			A) #pragma omp for private(elem, M, F, K, TL, i, MTnewL)
			B) #pragma omp for private(elem, M, F, K, TL, i, MTnewL) 
			B) reduction(+: MTnew[0:nn]) schedule(dynamic,512)
			for(int e=0; e<ne; e++)
			{
				elem = mesh->getElem(e);
				M = elem->getMptr();
				F = elem->getFptr();
				K = elem->getKptr();
				for(i=0; i<nen; i++)
				{
					TL[i] = T[elem->getConn(i)];
				}
				
				MTnewL[0] = M[0]*TL[0] + dT*(F[0]-(K[0]*TL[0]+...));
				MTnewL[1] = M[1]*TL[1] + dT*(F[1]-(K[3]*TL[0]+...));
				MTnewL[2] = M[2]*TL[2] + dT*(F[2]-(K[6]*TL[0]+...));
				
				
				// RHS is accumulated at local nodes
				A) #pragma omp critical
				MTnew[elem->getConn(0)] += MTnewL[0];
				A) #pragma omp critical
				MTnew[elem->getConn(1)] += MTnewL[1];
				A) #pragma omp critical
				MTnew[elem->getConn(2)] += MTnewL[2];
			}
			// Evaluate the new temperature on each node on partition level
			partialL2error = 0.0;
			globalL2error = 0.0;
			A) #pragma omp for private(pNode, massTmp, MT, Tnew)
			B) #pragma omp for private(pNode, massTmp, MT, Tnew) 
			B) reduction(+:partialL2error) schedule(dynamic,512)
			for(int i=0; i<nn; i++)
			{
				pNode = mesh->getNode(i);
				if(pNode->getBCtype() != 1)
				{
					massTmp = massG[i];
					MT = MTnew[i];
					Tnew = MT/massTmp;
					A) #pragma omp critical
					partialL2error += pow(T[i]-Tnew,2);
					T[i] = Tnew;
				}
			}
		}
		
		globalL2error = sqrt(partialL2error/nn);
\end{lstlisting}

\subsection*{Parallelization with MPI}

\section{Results}

\subsection{Serial Solver}

To run the finite element solver as efficient and fast a possible in parallel, it is important that the serial performance is optimized. Therefore, the effect on the runtime of multiple compiler flags is compared in this section.

\subsubsection*{R1 a)}
For the validation of the FEM solver, the solution of the coarse mesh is compared to the analytical solution in \refFig{fig::TemperatureDist}. The numeric solution is in good agreement with the analytical solution. Slight deviations are observed at the disk's center where the temperature distribution has it's global maximum. Accounting for large changes in the temperature gradient at the center of the disk. 

\begin{figure}[!htbp]
	\centering
	%\hspace*{0.8cm}
	\leavevmode
	\resizebox{0.8\width}{!}{\input{plots/serial/TemperatureDist.pgf}}
	\caption{Comparison of the steady-state temperature distribution of the analytical solution (refEQ) and the numerical solution on the coarse mesh}
	\label{fig::TemperatureDist}
\end{figure}

In order to decrease the serial runtime of the implemented solver, different compiler flags were tested and assessed by comparing their influence on the solver runtime. The code is compiled with the Intel C++ Compiler (v19.1). The compiled code is tested on a Intel Platinum 8160 which is based on Intel's Skylake architecture using the "medium" unstructured mesh with 21650 nodes resulting in 21332 triangular elements.

The investigated compiler flags are contained in \refTab{tab:SerialTimings}.
In the following subsection the individual compiler flags are briefly presented and explained: 
 
\subsubsection*{-O0}
The capital number behind the capital "O" refers to the optimization level and performs general optimizations by bundling several individual compiler flags. Using the base Level (0),  the compiler will disable all optimizations. This is useful for debugging (section R1 b).

\subsubsection*{-O1}
Optimization Level 1 includes the optimization for speed, but disables all optimizations that increase code size and affect the codes speed. It includes the analysis of data-flow, code motion, strength reduction and further performance enhancing options.

\subsubsection*{-O2}
The second optimization level includes vectorization, which allows for concurrent execution of the separate steps that are necessary to perform different basic mathematical operations on array-like data structures. Such as an element wise addition of an array. A scalar or "Von Neumann" processor would simply perform all four necessary steps for each index step by step. Where as a vector processor allows for concurrent operations. This prevents that instruction units are idle, while waiting on a task to be finished.

The option also enables the inlining of intrinsics and intra-file inlining of functions. Which means the replacement of the function call with the instructions inside of a certain function itself. This can cause significant performance gain, especially if functions are called multiple times inside the body of a loop.

\subsubsection*{-O3}
Performs O2 optimizations with higher thresholds and enables more aggressive loop transformations such as Fusion, Block-Unroll-and-Jam, and collapsing IF-statements. It can be used with the option "-ax" to force the compiler to do a more aggressive data dependency analysis and optimize for a certain CPU architecture. Where "x" refers to the CPU's instruction set.

\subsubsection*{-Ofast}
On Linux systems this sets "-O3 -no-prec-div -fp-model fast=2". This causes the compiler to enhance the code with alle possible optimizations of "-O3" and reduce the precision of floating-point divides.

\subsubsection*{-ax \textit{arch}}
Optimization of instruction set for a specific hardware architecure of the CPU. As the simulations were conducted on a two-socket node with two Intel Platinum 8160 with each 24 cores, based on the Skylake architecture. The shortest runtime was achieved with the flag "-axSKYLAKE-AVX512". Where AVX512 is the abbreviation for \textit{Advanced Vector Extensions} with 512-bit instructions and is an extension of the SIMD instruction set. 

\subsubsection*{-fp-model fast}
This flag controls the precision of floating-point operations. Using Level 2, enables a more aggressive optimization of floating-point calculations. This might decrease accuracy.

\subsubsection*{-ipo}
Enables the interprocedural optimization between files functions from different files may be inlined.

\subsubsection*{-unroll}
Performs loop unrolling which helps to exploit multiple instruction units, by increasing the stride length of the loop variable and performing the same operation on multiple array elements in one stride. 
\\
\citep{anmeyRWTHHPCClusterUser, behrLectureParallelComputing, QuickReferenceGuide, IntelCompilerClassic2022}
\\
\\
The runtimes that were measured for the different compiler flags are shown in \refTab{tab:SerialTimings}. For each flag, three measurements on a single core with a single thread where taken and the average time was determined. The shortest runtime is achieved by using the compiler flag combination "-O3 -axSKYLAKE-AVX512" resulting in an average runtime of 54.77 seconds which is 4.37 times faster that the not optimised code (-O0). For further investigations, assessing the parallel optimisation with OpenMP and MPI, the "-O3 -axSKYLAKE-AVX512" flag combination was used.

\renewcommand{\arraystretch}{2}
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{| p{5cm} | p{1.5cm} p{1.5cm} p{1.5cm} | p{1.5cm} | }
			\hline
			\hline
			compiler flag & time 1 & time 2 & time 3 & average time\\
			\hline
			-O0 (no optimisation) & 233.71	& 241.5	& 243.02 & 239.41 \\
			\hline
			-O1 & 62.41	& 63.71	& 62.76	& 62.96\\
			\hline
			-O2 & 56.96	& 55.39	& 57.11	& 56.49\\
			\hline 
			-O3 & 55.49	& 55.23	& 57.8	& 56.17\\
			\hline
			-O3 -axSSE4.2, SSSE3, SSE2 -fp-model fast=2 & 56.17	& 56.21	& 56.46	& 56.18\\
			\hline
			-O3 -fp-model fast=2 & 57.69 & 56.19 & 57.89 & 57.26\\
			\hline
			-O3 -axSKYLAKE-AVX512 & 53.99 & 54.06 & 56.25 & \textbf{54.77}\\
			\hline
			-O3 -ipo & 54.57 & 57.26 & 60.38 & 57.40\\
			\hline
			-unroll & 55.17	& 57.01	& 55.94	& 56.04\\
			\hline
			-Ofast & 55.43	& 56.17	& 56.27	& 55.96\\
			\hline
			\hline
		\end{tabular}
		\caption{\label{tab:SerialTimings}  Timings of the various compiler flags for serial optimisation, the shortest achieved runtime is printed in bold font}
	\end{center}
\end{table}
\renewcommand{\arraystretch}{1}

%\label{sec:R1b}
\subsubsection*{R1 b)}
A higher level of optimization can cause a decrease in the accuracy of the floating-point operations. For the validation of a certain code or high fidelity simulations, like the implemented FEM Solver, one is advised to minimize the numerical error, by using the highest level of accuracy. So the optimizations should not inflict with the desired results. Also for debugging purposes, optimization is not helpful as the compiler changes the code structure, when using more aggressive optimizations. To name a few effects: functions are inlined, loops are unrolled or fused together. This makes is very hard to search for bugs, when using debugging software.

\subsection{Parallel OpenMP Solver}

In this section the performance gain by parallelization with OpenMP of the two most time consuming loops is investigated (\refCode{Code:OpenMP}).  

\subsubsection*{R2 a}

First, two different OpenMP parallelization approaches are compared an discussed. The Approaches are denoted with "A)" and "B)" in the code extract in \refCode{Code:OpenMP}. Approach A) uses critical regions, which are executed by each thread serially for the computation of the RHS at element-level and the partial L2 errors. Where as approach B) is performing a reduction operation for the determination of MTnew and the partial L2 error, using a dynamic scheduling with a chunk size of 512 for both loops. 

To compare both approaches, the runtimes for the simulation of the temperature distribution on the coarse mesh is determined for 1, 2 and 4 threads for each parallelization approach. Three runtimes were measured per thread count and averaged. 
As presented in \refFig{fig::OpenMP1}, the runtimes of the approach A) (\refFig{fig::OpenMPA}) for different number of threads is in general significantly higher than the runtimes of approach B) (\refFig{fig::OpenMPB}), with a top runtime of 142.59 seconds for a run with 4 threads for approach A and a runtime of 1.01 seconds for approach B. The scaling behaviour of approach B was found to be as theoretically expected, as the runtime decreases with an increasing number of threads. Due to the very short runtimes the run with 2 threads was measured to be slower than a run with only a single thread. This requires further investigations as possible fluctuations in the runtime, for instance due to memory access, are in the order of the runtime itself. Approach A shows the opposite dependency of thread number and runtime. The runtime increases with an increasing number of threads. 

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h!]{.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				ybar,
				width = 0.85 * \textwidth,
				height = 8cm,
				major x tick style = transparent,
				bar width = 14pt,
				ymajorgrids = true,
				ylabel = {Runtime in [s]},
				symbolic x coords = {1 Thread, 2 Threads, 4 Threads},
				xtick = data,
				scaled y ticks = false,
				enlarge x limits = 0.25,
				ymin = 0,
				nodes near coords,
				node near coord style={black}
%				legend cell align=left,
%				legend style={
%					at={(1,1.05)},
%					anchor=south east,
%					column sep=1ex
%				}
				]
				\addplot [style={blue, fill=blue, mark=none}]
				coordinates{ (1 Thread, 8.34) (2 Threads, 68.32) (4 Threads, 142.59)};
			\end{axis}
		\end{tikzpicture}
		\caption{\label{fig::OpenMPA} OpenMP task A}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h!]{.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				ybar,
				width = 0.85 * \textwidth,
				height = 8cm,
				major x tick style = transparent,
				bar width = 14pt,
				ymajorgrids = true,
				ylabel = {Runtime in [s]},
				symbolic x coords = {1 Thread, 2 Threads, 4 Threads},
				xtick = data,
				scaled y ticks = false,
				enlarge x limits = 0.25,
				ymin = 0,
				nodes near coords,
				node near coord style={black}
%				legend cell align=left,
%				legend style={
%					at={(1,1.05)},
%					anchor=south east,
%					column sep=1ex
%				}
				]
				\addplot [style={red, fill=red, mark=none}]
				coordinates{ (1 Thread, 1.27) (2 Threads, 1.35) (4 Threads, 1.01)};
			\end{axis}
		\end{tikzpicture}
		\caption{\label{fig::OpenMPB} OpenMP task B}
	\end{subfigure}
\caption{\label{fig::OpenMP1} Timings for solver with different OpenMP optimisations using the coarse mesh}
\end{figure}

As the critical regions of approach A) are to be executed by only one thread at a time, the runtime will increase with an increasing number of threads, as observed in \refFig{fig::OpenMPA}. A large overhead is created due to the management of lock, to ensure that only one thread at a time is running through a critical region. This overhead increases with a higher number of threads. While one thread is executing a critical region the other threads have to wait for this specific thread to finish this section. Only if this thread has completely finished to computations inside a critical region the next thread is allowed to execute this region for its assigned loop indices. To prevent the overhead of multiple locks and to allow threads to work in parallel the reduction keyword for openMP is used. The reduction operation for the computation of the RHS at element level for each element node, allows each thread to first compute local sums of the each MTnew entry in parallel and then performs a serial summation of all local sums to obtain the value of an MTnew entry. But this serial summation is only computed once, compared to the critical region. The same procedure applies also for the computation of the partial L2 error. 

\subsubsection*{R2 b}

To further optimize the parallelization with OpenMP, different scheduling options and chunk sizes are investigated for approach B) (\refCode{Code:OpenMP}). Therefore simulations on the fine mesh with a constant number of 4 threads were conducted. Some unexplained behaviour was observed, when using scheduling options other than static for the second loop in which the partial L2 error are computed, where the L2 error gets initialised again with 0, which leads to an untimely convergence of the simulation. Therefore different scheduling options and chunk sizes were first only applied for the first loop (line 15 \refCode{Code:OpenMP}) for the computation of the RHS on element level. For the second loop (line 45 \refCode{Code:OpenMP}) a static scheduling with default chunk size was used. The results are presented in \refFig{fig::Scheduling}. 
In general the static scheduling performs best with a shortest runtime of 184.06 seconds for a chunk size of 256. In general larger runtimes are observed for the dynamic and guided scheduling options. For dynamic scheduling a larger chunk size of 512 iterations leads to shorter runtimes, where as for static and guided scheduling the medium chunk size of 256 iterations performs best. The runtime of the auto scheduling is at 220.97 seconds, which is marked in grey in \refFig{fig::Scheduling}.

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			width = 0.85 * \textwidth,
			height = 8cm,
			major x tick style = transparent,
			bar width = 14pt,
			ymajorgrids = true,
			ylabel = {Runtime in [s]},
			symbolic x coords = {static, dynamic, guided, auto},
			xtick = data,
			scaled y ticks = false,
			enlarge x limits = 0.25,
			ymin = 0,
			nodes near coords,
			node near coord style={black, anchor=east, rotate=90},
			legend cell align=left,
			legend style={
					at={(1,1.05)},
					anchor=south east,
					column sep=1ex
				}
			]
			\addplot 
			coordinates{ (static, 184.81) (dynamic, 241.53) (guided, 222.85)};
			
			\addplot
			coordinates{ (static, 184.06) (dynamic, 237.72) (guided, 226.06)};
			
			\addplot
			coordinates{ (static, 188.59) (dynamic, 232.95) (guided, 216.26)};
			
			\addplot
			coordinates{(auto, 220.97)};
			
		\end{axis}
	\end{tikzpicture}
	\caption{\label{fig::Scheduling} Comparison of different scheduling options (static, dynamic, guided, auto) for different chunk sizes(128, 256, 512) in first parallelised loop for 4 threads using the finest mesh}
\end{figure}

In addition to the introductory investigations regarding the different scheduling options, static scheduling was inspected further. Therefore the reduction operations for the parallelisation of the first and second loop are both equipped with static scheduling and the effects of different chunk sizes are compared in \refFig{fig::OpenMPStatic}. The runtime is decreasing with an increasing chunk size. The shortest runtime is observed for the default option in scheduling with 177.21 seconds on average, which divides the number of loop iterations by the number of threads. This results in a chunk size of 21164 iterations for the first loop and in a chunk size of 21322.5 iterations on average for the second loop for the fine mesh with 85290 nodes and 84656 elements. Thus, the static scheduling with the default chunk size is used for the calculation of speed-up and efficiency for the different meshes in the following section.

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			width = 0.85 * \textwidth,
			height = 8cm,
			major x tick style = transparent,
			bar width = 14pt,
			ymajorgrids = true,
			ylabel = {Runtime in [s]},
			xlabel = {chunk sizes},
			symbolic x coords = {128,256,512,default},
			xtick = data,
			scaled y ticks = false,
			enlarge x limits = 0.25,
			ymin = 0,
			nodes near coords,
			node near coord style={black}
			]
			\addplot
			coordinates{ (128, 190.84) (256, 184.83) (512, 184.48) (default, 177.21)};
		\end{axis}
	\end{tikzpicture}
	\caption{\label{fig::OpenMPStatic} Runtimes for parallelization of loop in line 15 and loop in line 45 in \refCode{Code:OpenMP} with static scheduling and the same option for chunk size }
\end{figure}

\subsubsection*{R2 c}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/OpenMP/SpeedUp.pgf}}
		\caption{\label{fig::SPOpenMP} Speed-up}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/OpenMP/Efficiency.pgf}}
		\caption{\label{fig::EffOpenMP} Efficiency}
	\end{subfigure}
	\caption{\label{fig::EffSPOpenMP} Speed-up and efficiencies for coarse, medium and fine mesh for different numbers of threads}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_coarse.pgf}}
		\caption{\label{fig::Rcoarse} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_medium.pgf}}
		\caption{\label{fig::Rmedium} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_fine.pgf}}
		\caption{\label{fig::Rfine} fine mesh}
	\end{subfigure}
	\caption{\label{fig::Runtime} Runtime for coarse, medium and fine mesh for different number of threads}
\end{figure}

%\renewcommand{\arraystretch}{2}
%\begin{table}[h!]
%	\begin{center}
%	\begin{subtable}{0.8\textwidth}
%		\begin{tabular}{| p{5cm} | p{1.5cm} p{1.5cm} p{1.5cm} | p{1.5cm} | }
%			\hline
%			\hline
%			number of threads & time1 & tim2 & time3 & average time \\
%			\hline
%			1 & 1.30684	& 1.24179 & 1.28846 & 1.28\\
%			\hline
%			2 &	1.43304	& 1.51202 & 1.63023	& 1.53\\
%			\hline
%			4 & 1.18422	& 1.07168 &	1.26715	& 1.17\\
%			\hline
%			6 &	1.19 & 1.14537 & 1.19051 & 1.17\\
%			\hline
%			8	1,22	1,1795	1,13683	1,18
%			\hline
%			12	1,15932	1,12042	1,20255	1,16
%			\hline
%			\hline
%		\end{tabular}
%		\caption{\label{tab:sSerialTimings}  Timings of the various compiler flags for serial optimisation}
%	\end{subtable}
%	
%	\caption{\label{tab:sSesrialTimings}  Timings of the various compiler flags for serial optimisation}
%	\end{center}
%\end{table}
%\renewcommand{\arraystretch}{1}

\subsubsection*{R2 d}
- look into presentation

\subsection{Parallel MPI Solver}

\subsubsection*{R3 a}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/MPI/Runtime_coarse.pgf}}
		\caption{\label{fig::RcoarseMPI} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/MPI/Runtime_medium.pgf}}
		\caption{\label{fig::RmediumMPI} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/MPI/Runtime_fine.pgf}}
		\caption{\label{fig::RfineMPI} fine mesh}
	\end{subfigure}
	\caption{\label{fig::RuntimeMPI} Runtime for coarse, medium and fine mesh for different number of cores}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/SpeedUp_coarse.pgf}}
		\caption{\label{fig::SPcoarseMPI} Speed-up coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/Efficiency_coarse.pgf}}
		\caption{\label{fig::EffcoarseMPI} Efficiency coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/SpeedUp_medium.pgf}}
		\caption{\label{fig::SPmediumMPI} Speed-up medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/Efficiency_medium.pgf}}
		\caption{\label{fig::EffmediumMPI} Efficiency medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/SpeedUp_fine.pgf}}
		\caption{\label{fig::SPfineMPI} Speed-up fine mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/MPI/Efficiency_fine.pgf}}
		\caption{\label{fig::EfffineMPI} Efficiency fine mesh}
	\end{subfigure}
	\caption{\label{fig::MPI} Speed-up and efficiency for coarse, medium and fine mesh for different number of cores}
\end{figure}

\subsubsection*{R3 b}

\begin{figure}[h!]
	\centering
	\resizebox{0.25\width}{!}{\includegraphics{plots/MPI/partitioning/partitioning.png}}
	\caption{\label{fig::partition} Partitioning of medium mesh using 8 cores}
\end{figure}

%TODO:

- Plots größer

\subsubsection*{R3 c}

\subsubsection*{R3 d}

\section{Discussion}



\section{Conclusion}

\blindtext

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
