\documentclass[a4paper, 11pt, oneside]{scrartcl}
\usepackage{lpp}

\usepackage{mathcomp}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{epstopdf}
\usepackage{makeidx}
\usepackage{mathtools} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{etoolbox}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes.geometric, arrows, positioning, patterns}
%\usepackage[caption=false,subrefformat=parens,labelformat=parens]{subfig}
\usepackage{nomencl}
\usepackage{doi}

\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}%   partielle Ableitung
\newcommand{\tensor}[1]{\overline{\overline{#1}}}%      Tensor
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\refFig}[1]{Fig. \ref{#1}}
\newcommand{\refEq}[1]{Eq. (\ref{#1})}
\newcommand{\refEqs}[2]{Eqs. (\ref{#1}) and (\ref{#2})}
\newcommand{\refSec}[1]{Sec. \ref{#1}}
\newcommand{\refTab}[1]{Tab. \ref{#1}}
\newcommand{\refCh}[1]{Ch. \ref{#1}}


\begin{document}

\mtitle{Final Report}

\mauthor{Parallel Computing for Computational Mechanics}

\maff{Johannes Grafen 380149}{johannes.grafen@rwth-aachen.de}

\mabstract{\blindtext}

\section{Introduction}

%\nocite{midsummer}

The parallelization of code for the simulation of complex mechanical systems is a key-topic in modern computational mechanics. Constantly increasing computing power of high-performance clusters allow for the simulation of large and complex systems. This requires an efficient parallelization approach, where different parallelization paradigms can be of use. \\

In this report the parallelization of a finite element code for the simulation of the stationary temperature distribution on a two-dimensional disk is investigated. For the shared memory approach the OpenMP library is used, where as the distributed memory approach is assessed by using a one-sided parallelization approach with MPI. The different parallelization approaches are discussed and compared for different number of CPUs. This report is structured as follows. In the second chapter \textit{Theory and Methods} are presented. The \textit{Implementation and Validation} of the presented problem is discussed in chapter 3. The results of the optimization of the runtime of the code is illustrated in chapter 4 \textit{Results} and the assessment of the different optimization techniques is discussed in chapter 5 \textit{Discussion}. Finally, this report is closed with a \textit{Conclusion} in chapter 6.

serial optimization is performed by  

\section{Theory and Methods}


- describe different meshes

\section{Implementation and Validation}

\subsection*{Parallelization with OpenMP}

\subsection*{Parallelization with MPI}

\section{Results}

\subsection{Serial Solver}

To run the finite element solver as efficient and fast a possible in parallel, it is important that the serial performance is optimized. Therefore, the effect on the runtime of multiple compiler flags is compared in this section.

\subsubsection*{R1 a)}
For the validation of the FEM solver, the solution of the coarse mesh is compared to the analytical solution in \refFig{fig::TemperatureDist}. The numeric solution is in good agreement with the analytical solution. Slight deviations are observed at the disk's center where the temperature distribution has it's global maximum. Accounting for large changes in the temperature gradient at the center of the disk. 

\begin{figure}[!htbp]
	\centering
	%\hspace*{0.8cm}
	\leavevmode
	\resizebox{0.8\width}{!}{\input{plots/serial/TemperatureDist.pgf}}
	\caption{Comparison of the steady-state temperature distribution of the analytical solution (refEQ) and the numerical solution on the coarse mesh}
	\label{fig::TemperatureDist}
\end{figure}

In order to decrease the serial runtime of the implemented solver, different compiler flags were tested and assessed by comparing their influence on the solver runtime. The code is compiled with the Intel C++ Compiler (v19.1). The compiled code is tested on a Intel Platinum 8160 which is based on Intel's Skylake architecture using the "medium" unstructured mesh with 21650 nodes resulting in 21332 triangular elements.

The investigated compiler flags are contained in \refTab{tab:SerialTimings}.
In the following subsection the individual compiler flags are briefly presented and explained: 
 
\subsubsection*{-O0}
The capital number behind the capital "O" refers to the optimization level and performs general optimizations by bundling several individual compiler flags. Using the base Level (0),  the compiler will disable all optimizations. This is useful for debugging (section R1 b).

\subsubsection*{-O1}
Optimization Level 1 includes the optimization for speed, but disables all optimizations that increase code size and affect the codes speed. It includes the analysis of data-flow, code motion, strength reduction and further performance enhancing options.

\subsubsection*{-O2}
The second optimization level includes vectorization, which allows for concurrent execution of the separate steps that are necessary to perform different basic mathematical operations on array-like data structures. Such as an element wise addition of an array. A scalar or "Von Neumann" processor would simply perform all four necessary steps for each index step by step. Where as a vector processor allows for concurrent operations. This prevents that instruction units are idle, while waiting on a task to be finished.

The option also enables the inlining of intrinsics and intra-file inlining of functions. Which means the replacement of the function call with the instructions inside of a certain function itself. This can cause significant performance gain, especially if functions are called multiple times inside the body of a loop.

\subsubsection*{-O3}
Performs O2 optimizations with higher thresholds and enables more aggressive loop transformations such as Fusion, Block-Unroll-and-Jam, and collapsing IF-statements. It can be used with the option "-ax" to force the compiler to do a more aggressive data dependency analysis and optimize for a certain CPU architecture. Where "x" refers to the CPU's instruction set.

\subsubsection*{-Ofast}
On Linux systems this sets "-O3 -no-prec-div -fp-model fast=2". This causes the compiler to enhance the code with alle possible optimizations of "-O3" and reduce the precision of floating-point divides.

\subsubsection*{-ax \textit{arch}}
Optimization of instruction set for a specific hardware architecure of the CPU. As the simulations were conducted on a two-socket node with two Intel Platinum 8160 with each 24 cores, based on the Skylake architecture. The shortest runtime was achieved with the flag "-axSKYLAKE-AVX512". Where AVX512 is the abbreviation for \textit{Advanced Vector Extensions} with 512-bit instructions and is an extension of the SIMD instruction set. 

\subsubsection*{-fp-model fast}
This flag controls the precision of floating-point operations. Using Level 2, enables a more aggressive optimization of floating-point calculations. This might decrease accuracy.

\subsubsection*{-ipo}
Enables the interprocedural optimization between files functions from different files may be inlined.

\subsubsection*{-unroll}
Performs loop unrolling which helps to exploit multiple instruction units, by increasing the stride length of the loop variable and performing the same operation on multiple array elements in one stride. 
\\
\citep{anmeyRWTHHPCClusterUser, behrLectureParallelComputing, QuickReferenceGuide, IntelCompilerClassic2022}
\\
\\
The runtimes that were measured for the different compiler flags are shown in \refTab{tab:SerialTimings}. For each flag, three measurements on a single core with a single thread where taken and the average time was determined. The shortest runtime is achieved by using the compiler flag combination "-O3 -axSKYLAKE-AVX512" resulting in an average runtime of 54.77 seconds which is 4.37 times faster that the not optimised code (-O0). For further investigations, assessing the parallel optimisation with OpenMP and MPI, the "-O3 -axSKYLAKE-AVX512" flag combination was used.

\renewcommand{\arraystretch}{2}
\begin{table}[h!]
	\begin{center}
		\begin{tabular}{| p{5cm} | p{1.5cm} p{1.5cm} p{1.5cm} | p{1.5cm} | }
			\hline
			\hline
			compiler flag & time 1 & time 2 & time 3 & average time\\
			\hline
			-O0 (no optimisation) & 233.71	& 241.5	& 243.02 & 239.41 \\
			\hline
			-O1 & 62.41	& 63.71	& 62.76	& 62.96\\
			\hline
			-O2 & 56.96	& 55.39	& 57.11	& 56.49\\
			\hline 
			-O3 & 55.49	& 55.23	& 57.8	& 56.17\\
			\hline
			-O3 -axSSE4.2, SSSE3, SSE2 -fp-model fast=2 & 56.17	& 56.21	& 56.46	& 56.18\\
			\hline
			-O3 -fp-model fast=2 & 57.69 & 56.19 & 57.89 & 57.26\\
			\hline
			-O3 -axSKYLAKE-AVX512 & 53.99 & 54.06 & 56.25 & \textbf{54.77}\\
			\hline
			-O3 -ipo & 54.57 & 57.26 & 60.38 & 57.40\\
			\hline
			-unroll & 55.17	& 57.01	& 55.94	& 56.04\\
			\hline
			-Ofast & 55.43	& 56.17	& 56.27	& 55.96\\
			\hline
			\hline
		\end{tabular}
		\caption{\label{tab:SerialTimings}  Timings of the various compiler flags for serial optimisation, the shortest achieved runtime is printed in bold font}
	\end{center}
\end{table}
\renewcommand{\arraystretch}{1}

%\label{sec:R1b}
\subsubsection*{R1 b)}
A higher level of optimization can cause a decrease in the accuracy of the floating-point operations. For the validation of a certain code or high fidelity simulations, like the implemented FEM Solver, one is advised to minimize the numerical error, by using the highest level of accuracy. So the optimizations should not inflict with the desired results. Also for debugging purposes, optimization is not helpful as the compiler changes the code structure, when using more aggressive optimizations. To name a few effects: functions are inlined, loops are unrolled or fused together. This makes is very hard to search for bugs, when using debugging software.

\subsection{Parallel OpenMP Solver}

\subsubsection*{R2 a}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h!]{.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				ybar,
				width = 0.85 * \textwidth,
				height = 8cm,
				major x tick style = transparent,
				bar width = 14pt,
				ymajorgrids = true,
				ylabel = {Runtime in [s]},
				symbolic x coords = {1 Thread, 2 Threads, 4 Threads},
				xtick = data,
				scaled y ticks = false,
				enlarge x limits = 0.25,
				ymin = 0,
				nodes near coords,
				node near coord style={black}
%				legend cell align=left,
%				legend style={
%					at={(1,1.05)},
%					anchor=south east,
%					column sep=1ex
%				}
				]
				\addplot [style={blue, fill=blue, mark=none}]
				coordinates{ (1 Thread, 8.06) (2 Threads, 63.4) (4 Threads, 140.76)};
			\end{axis}
		\end{tikzpicture}
		\caption{\label{fig::OpenMPA} OpenMP task A}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h!]{.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				ybar,
				width = 0.85 * \textwidth,
				height = 8cm,
				major x tick style = transparent,
				bar width = 14pt,
				ymajorgrids = true,
				ylabel = {Runtime in [s]},
				symbolic x coords = {1 Thread, 2 Threads, 4 Threads},
				xtick = data,
				scaled y ticks = false,
				enlarge x limits = 0.25,
				ymin = 0,
				nodes near coords,
				node near coord style={black}
%				legend cell align=left,
%				legend style={
%					at={(1,1.05)},
%					anchor=south east,
%					column sep=1ex
%				}
				]
				\addplot [style={red, fill=red, mark=none}]
				coordinates{ (1 Thread, 1.33) (2 Threads, 1.26) (4 Threads, 1.12)};
			\end{axis}
		\end{tikzpicture}
		\caption{\label{fig::OpenMPB} OpenMP task B}
	\end{subfigure}
\caption{\label{fig::OpenMP1} Timings for solver with different OpenMP optimisations using the coarse mesh}
\end{figure}

\subsubsection*{R2 b}

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			width = 0.85 * \textwidth,
			height = 8cm,
			major x tick style = transparent,
			bar width = 14pt,
			ymajorgrids = true,
			ylabel = {Runtime in [s]},
			symbolic x coords = {static, dynamic, guided, auto},
			xtick = data,
			scaled y ticks = false,
			enlarge x limits = 0.25,
			ymin = 0,
			nodes near coords,
			node near coord style={black, anchor=east, rotate=90},
			legend cell align=left,
			legend style={
					at={(1,1.05)},
					anchor=south east,
					column sep=1ex
				}
			]
			\addplot 
			coordinates{ (static, 184.81) (dynamic, 241.53) (guided, 222.85)};
			
			\addplot
			coordinates{ (static, 184.06) (dynamic, 237.72) (guided, 226.06)};
			
			\addplot
			coordinates{ (static, 188.59) (dynamic, 232.95) (guided, 216.26)};
			
			\addplot
			coordinates{(auto, 220.97)};
			
		\end{axis}
	\end{tikzpicture}
	\caption{\label{fig::Scheduleing} Comparison of different scheduling options (static, dynamic, guided, auto) for different chunk sizes(128, 256, 512) in first parallelised loop for 4 threads using the finest mesh}
\end{figure}

\subsubsection*{R2 c}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/OpenMP/SpeedUp.pgf}}
		\caption{\label{fig::SPOpenMP} Speed-up}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\resizebox{0.5\width}{!}{\input{plots/OpenMP/Efficiency.pgf}}
		\caption{\label{fig::EffOpenMP} Efficiency}
	\end{subfigure}
	\caption{\label{fig::EffSPOpenMP} Speed-up and efficiencies for coarse, medium and fine mesh for different numbers of threads}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_coarse.pgf}}
		\caption{\label{fig::Rcoarse} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_medium.pgf}}
		\caption{\label{fig::Rmedium} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.8\textwidth}
		\centering
		\resizebox{0.6\width}{!}{\input{plots/OpenMP/Runtime_fine.pgf}}
		\caption{\label{fig::Rfine} fine mesh}
	\end{subfigure}
	\caption{\label{fig::Runtime} Runtime for coarse, medium and fine mesh for different number of threads}
\end{figure}

%\renewcommand{\arraystretch}{2}
%\begin{table}[h!]
%	\begin{center}
%	\begin{subtable}{0.8\textwidth}
%		\begin{tabular}{| p{5cm} | p{1.5cm} p{1.5cm} p{1.5cm} | p{1.5cm} | }
%			\hline
%			\hline
%			number of threads & time1 & tim2 & time3 & average time \\
%			\hline
%			1 & 1.30684	& 1.24179 & 1.28846 & 1.28\\
%			\hline
%			2 &	1.43304	& 1.51202 & 1.63023	& 1.53\\
%			\hline
%			4 & 1.18422	& 1.07168 &	1.26715	& 1.17\\
%			\hline
%			6 &	1.19 & 1.14537 & 1.19051 & 1.17\\
%			\hline
%			8	1,22	1,1795	1,13683	1,18
%			\hline
%			12	1,15932	1,12042	1,20255	1,16
%			\hline
%			\hline
%		\end{tabular}
%		\caption{\label{tab:sSerialTimings}  Timings of the various compiler flags for serial optimisation}
%	\end{subtable}
%	
%	\caption{\label{tab:sSesrialTimings}  Timings of the various compiler flags for serial optimisation}
%	\end{center}
%\end{table}
%\renewcommand{\arraystretch}{1}

\subsubsection*{R2 d}
- look into presentation

\subsection{Parallel MPI Solver}

\subsubsection*{R3 a}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Runtime_coarse.pgf}}
		\caption{\label{fig::RcoarseMPI} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Runtime_medium.pgf}}
		\caption{\label{fig::RmediumMPI} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Runtime_fine.pgf}}
		\caption{\label{fig::RfineMPI} fine mesh}
	\end{subfigure}
	\caption{\label{fig::RuntimeMPI} Runtime for coarse, medium and fine mesh for different number of cores}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/SpeedUp_coarse.pgf}}
		\caption{\label{fig::SPcoarseMPI} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/SpeedUp_medium.pgf}}
		\caption{\label{fig::SPmediumMPI} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/SpeedUp_fine.pgf}}
		\caption{\label{fig::SPfineMPI} fine mesh}
	\end{subfigure}
	\caption{\label{fig::SpeedUpMPI} Speed-up for coarse, medium and fine mesh for different number of cores}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Efficiency_coarse.pgf}}
		\caption{\label{fig::EffcoarseMPI} coarse mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Efficiency_medium.pgf}}
		\caption{\label{fig::EffmediumMPI} medium mesh}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.32\textwidth}
		\centering
		\resizebox{0.33\width}{!}{\input{plots/MPI/Efficiency_fine.pgf}}
		\caption{\label{fig::EfffineMPI} fine mesh}
	\end{subfigure}
	\caption{\label{fig::EffMPI} Efficiency for coarse, medium and fine mesh for different number of cores}
\end{figure}

\subsubsection*{R3 b}

\begin{figure}[h!]
	\centering
	\resizebox{0.25\width}{!}{\includegraphics{plots/MPI/partitioning/partitioning.png}}
	\caption{\label{fig::partition} Partitioning of medium mesh using 8 cores}
\end{figure}

\subsubsection*{R3 c}

\subsubsection*{R3 d}

\section{Discussion}



\section{Conclusion}

\blindtext

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
